dataset:
  args:
    root: data/
    train: true
  type: RQP4
model:
  args:
    hid_features:
    - 32
    - 16
    proj_loss: L1Loss
    proj_loss_wght: 0.5
  type: MahalanobisAutoencoder
optimizer:
  args:
    amsgrad: true
    lr: 0.001
    weight_decay: 0
  type: Adam
trainer:
  args:
    batch_size: 16
    checkpoint_freq: 50
    checkpoint_start: 100
    max_epochs: 350
    pruning:
      args:
        amount: 0.1
        parameters:
        - encoder.layer1.weight
        - encoder.layer1.bias
        - encoder.layer2.weight
        - encoder.layer2.bias
        - decoder.layer2.weight
        - decoder.layer2.bias
        - decoder.layer3.weight
        - decoder.layer3.bias
        pruning_method: L1Unstructured
      schedule:
      - 10
      - 15
      target_sparsity: 0.8
      type: global_unstructured
    shuffle: true
    tb_logs:
    - - plot_slow_latent_correlation
      - {}
    valid_split: 0.3
  type: PrunedTrainer
